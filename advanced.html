<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.11"/>
<title>Canopy: Advanced Usage - Defining Your Own Models</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { init_search(); });
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Canopy
   &#160;<span id="projectnumber">1.0</span>
   </div>
   <div id="projectbrief">The header-only random forests library</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.11 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li class="current"><a href="pages.html"><span>Related&#160;Pages</span></a></li>
      <li><a href="namespaces.html"><span>Namespaces</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="index.html">Overview</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Advanced Usage - Defining Your Own Models </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#choose_label">Choosing A Label Type</a></li>
<li class="level1"><a href="#node_dist">Defining Your Own Node Distribution</a></li>
<li class="level1"><a href="#output_dist">Defining Your Own Output Distribution</a></li>
<li class="level1"><a href="#custom_forest">Defining Your Own Forest Model</a></li>
</ul>
</div>
<div class="textblock"><p>If you want to define your own model using canopy, there are up to four steps you need to complete:</p>
<ul>
<li>Choose the label type of model.</li>
<li>Define the node distribution class you wish the model to use.</li>
<li>Define the output distribution class you wish the model to use. You only need to do this if you want to perform the <b> distribution prediction </b> task.</li>
<li>Define the forest model itself.</li>
</ul>
<p>These three tasks must be completed in this order because each depends on the previous. However in some situations it may be possible to re-use one of the node or output distributions already defined in canopy, in which case you can that step. Furthermore, in some situations the node distribution and output distributions may be the same (this is the case for the <a class="el" href="classcanopy_1_1classifier.html" title="Implements a random forest classifier model to predict a discrete output label. ">canopy::classifier</a> and <a class="el" href="classcanopy_1_1circular_regressor.html" title="Implements a random forest classifier model to predict a circular-valued output label. ">canopy::circularRegressor</a> models). In this case you just need to implement the behaviour of both models within one class.</p>
<p>The next four sections describe this process in detail. You should read these and read the existing code for <a class="el" href="classcanopy_1_1classifier.html" title="Implements a random forest classifier model to predict a discrete output label. ">canopy::classifier</a> and <a class="el" href="classcanopy_1_1circular_regressor.html" title="Implements a random forest classifier model to predict a circular-valued output label. ">canopy::circularRegressor</a> along with <a class="el" href="classcanopy_1_1discrete_distribution.html" title="A distribution that defines the probabilities over a number of discrete (integer-valued) class labels...">canopy::discreteDistribution</a> and <a class="el" href="classcanopy_1_1von_mises_distribution.html" title="A distribution that defines the probabilities over a circular-valued label. ">canopy::vonMisesDistribution</a> in order to understand how to build your own forest model.</p>
<h1><a class="anchor" id="choose_label"></a>
Choosing A Label Type</h1>
<p>Firstly, you need to choose the data type that you want your forest to predict In principle, this can be any data type. For example, the label type of the <a class="el" href="classcanopy_1_1classifier.html" title="Implements a random forest classifier model to predict a discrete output label. ">canopy::classifier</a> is <code>int</code>, and the label type of the <a class="el" href="classcanopy_1_1circular_regressor.html" title="Implements a random forest classifier model to predict a circular-valued output label. ">canopy::circularRegressor</a> is <code>float</code>.</p>
<p>For the sake of this tutorial, let's assume that we've chosen a label type of <code>myLabelType</code>.</p>
<h1><a class="anchor" id="node_dist"></a>
Defining Your Own Node Distribution</h1>
<p>The node distribution class defines the distribution that is stored in the leaf node of each tree in the forest. Conceptually, it should capture the behaviour of a probability distribution over the label type (<code>myLabelType</code> in our example).</p>
<p>There are a number of methods that the class <b>must</b> define in order to be used as a node distribution within canopy. You are of course free to add other methods or properties to give the class the behaviour you need.</p>
<p>The following example gives the required layout of the class. It might be easier for you to copy this file, change the type names, and fill in the blanks:</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;fstream&gt;</span></div><div class="line"></div><div class="line"><span class="keyword">class </span>myNodeDist</div><div class="line">{</div><div class="line">    <span class="keyword">public</span>:</div><div class="line"></div><div class="line">        <span class="keyword">template</span> &lt;<span class="keyword">class</span> TLabelIterator, <span class="keyword">class</span> TIdIterator&gt;</div><div class="line">        <span class="keywordtype">void</span> fit(TLabelIterator first_label, TLabelIterator last_label, TIdIterator first_id);</div><div class="line">        {</div><div class="line">            <span class="comment">/* Function used to fit the distribution to training data during</span></div><div class="line"><span class="comment">            forest training. The data are passed in using iterators pointing</span></div><div class="line"><span class="comment">            to the set of labels to fit to, and their IDs. In most cases, the</span></div><div class="line"><span class="comment">            IDs will be unused and only the labels will be relevant.</span></div><div class="line"><span class="comment"></span></div><div class="line"><span class="comment">            Due to the way the function is called by the randomForestBase class,</span></div><div class="line"><span class="comment">            TLabelIterator will be a random access iterator type (supports []</span></div><div class="line"><span class="comment">            syntax) that dereferences to myLabelType.</span></div><div class="line"><span class="comment">            */</span></div><div class="line">        }</div><div class="line"></div><div class="line"></div><div class="line">        <span class="keywordtype">void</span> printOut(std::ofstream&amp; stream)<span class="keyword"> const</span></div><div class="line"><span class="keyword">        </span>{</div><div class="line">            <span class="comment">/* Output parameters to &#39;stream&#39; that can later be used by</span></div><div class="line"><span class="comment">            readIn() to fully reconstruct the distribution.</span></div><div class="line"><span class="comment">            This will probably involve reocrding parameters like mean and</span></div><div class="line"><span class="comment">            variance, and possibly other information.</span></div><div class="line"><span class="comment"></span></div><div class="line"><span class="comment">            This will be called by randomForestBase when storing the model</span></div><div class="line"><span class="comment">            to a file. */</span></div><div class="line">        }</div><div class="line"></div><div class="line">        <span class="keywordtype">void</span> readIn(std::ifstream&amp; stream)</div><div class="line">        {</div><div class="line">            <span class="comment">/* Read in parameters from &#39;stream&#39; and use store them.</span></div><div class="line"><span class="comment">            This must match the format written by printOut()</span></div><div class="line"><span class="comment"></span></div><div class="line"><span class="comment">            This will be called by randomForestBase when storing the model</span></div><div class="line"><span class="comment">            to a file.*/</span></div><div class="line">        }</div><div class="line"></div><div class="line">        <span class="keyword">template</span> &lt;<span class="keyword">class</span> TId&gt;</div><div class="line">        <span class="keywordtype">float</span> pdf(<span class="keyword">const</span> myLabelType x, <span class="keyword">const</span> TId <span class="keywordtype">id</span>)<span class="keyword"> const</span></div><div class="line"><span class="keyword">        </span>{</div><div class="line">            <span class="comment">/* Return the probability of label x under the distribution</span></div><div class="line"><span class="comment">            Note that the id paramater will be unused in many cases, as the</span></div><div class="line"><span class="comment">            probability will not depend on the ID, only the label.</span></div><div class="line"><span class="comment"></span></div><div class="line"><span class="comment">            This is used by randomForestBase to perform the probability evaluation</span></div><div class="line"><span class="comment">            task. */</span></div><div class="line">        }</div><div class="line"></div><div class="line">        <span class="comment">// Use operator&lt;&lt; to print to the file stream</span></div><div class="line">        <span class="keyword">friend</span> std::ofstream&amp; operator&lt;&lt; (std::ofstream&amp; stream, <span class="keyword">const</span> myNodeDist&amp; dist)</div><div class="line">        {</div><div class="line">            dist.printOut(stream); <span class="keywordflow">return</span> stream;</div><div class="line">        }</div><div class="line"></div><div class="line">        <span class="comment">//Use operator&gt;&gt; to read from the file stream</span></div><div class="line">        <span class="keyword">friend</span> std::ifstream&amp; operator&gt;&gt; (std::ifstream&amp; stream, myNodeDist&amp; dist)</div><div class="line">        {</div><div class="line">            dist.readIn(stream); <span class="keywordflow">return</span> stream;</div><div class="line">        }</div><div class="line"></div><div class="line">    <span class="keyword">protected</span>:</div><div class="line">        </div><div class="line">        <span class="comment">// Distribution parameters etc</span></div><div class="line"></div><div class="line">};</div></div><!-- fragment --><h1><a class="anchor" id="output_dist"></a>
Defining Your Own Output Distribution</h1>
<p>The output distribution class defines the distribution that is created as the result of the <b>distribution prediction</b> task. It combines (in some sense that you can define) the node distributions reached in each forest for a given data point to produce a new distribution.</p>
<p>The class is required to have the layout in the following example:</p>
<div class="fragment"><div class="line"><span class="keyword">class </span>myOutputDist</div><div class="line">{</div><div class="line">    <span class="keyword">public</span>:</div><div class="line"></div><div class="line">        <span class="keyword">template</span> &lt;<span class="keyword">class</span> TId&gt;</div><div class="line">        <span class="keywordtype">void</span> combineWith(<span class="keyword">const</span> myNodeDist&amp; dist, <span class="keyword">const</span> TId <span class="keywordtype">id</span>)</div><div class="line">        {</div><div class="line">            <span class="comment">/* Update the distribution to reflect the effect of combining it</span></div><div class="line"><span class="comment">            with a node distribution &#39;dist&#39; */</span></div><div class="line">        }</div><div class="line"></div><div class="line">        <span class="keywordtype">void</span> normalise()</div><div class="line">        {</div><div class="line">            <span class="comment">/* Normalise the distribution after combining with several node</span></div><div class="line"><span class="comment">            distributions to ensure a valid distribution */</span></div><div class="line">        }</div><div class="line"></div><div class="line">        <span class="keywordtype">void</span> reset()</div><div class="line">        {</div><div class="line">            <span class="comment">/* Clear the results of all previous combinations to give a</span></div><div class="line"><span class="comment">            distribution that can be used to start the process on fresh data */</span></div><div class="line">        }</div><div class="line"></div><div class="line">    <span class="keyword">protected</span>:</div><div class="line"></div><div class="line">        <span class="comment">// Distribution parameters etc</span></div><div class="line">};</div></div><!-- fragment --><p>To make sense of the tasks that should be performed by the three methods, it is helpful to understand the order in which they are called by the <a class="el" href="classcanopy_1_1random_forest_base.html" title="Base class for random forests models from which all specific models are derived using CRTP...">canopy::randomForestBase</a> class. Recall from the <a class="el" href="basic.html">basic usage</a> instructions that the user code creates the output distribution object and passes it to the forest's <a class="el" href="classcanopy_1_1random_forest_base.html#a99ba77f1b521fc6992a8841294caedd1" title="Predict the output distribution for a number of IDs. ">canopy::randomForestBase::predictDistSingle()</a> or <a class="el" href="classcanopy_1_1random_forest_base.html#a1863eb32b1ae355c52c53669dd543593" title="Predict the output distribution for a number of IDs. ">canopy::randomForestBase::predictDistGroupwise()</a> method (via an iterator). Then that method calls uses the output distribution's methods as follows:</p>
<ul>
<li>First the <code>reset()</code> method is called. This should clear any existing data from the class such that it is ready for use with the combineWith method. This can ensure for example that if the user passes the same output distribution object to two calls of the <code>predictDistSingle/predictDistGroupwise</code> method, the second call overwrites any information from the first.</li>
<li>Next the forest finds which leaf nodes the input ID reaches. It then calls the <code>combineWith()</code> method of the output distribution once for each of these node distributions (i.e. once for each of the trees in the forest), passing a reference to the relevant node distribution object. The method should update the parameters of the output distribution object to reflect the inclusion of that leaf node. Note that the node distributions are passed in the order they appear in the list of trees in the forest, and in a single thread (i.e. you do not need to worry about data races within the <code>combineWith()</code> method).</li>
<li>After the <code>combineWith()</code> method has been called for all the trees in the forest, the <code>normalise()</code> method is called once. This can therefore be used to ensure that the resulting parameters represent a valid distribution.</li>
</ul>
<h1><a class="anchor" id="custom_forest"></a>
Defining Your Own Forest Model</h1>
<p>In order to define your own forest model you need to define a class whose layout follows the example below:</p>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">unsigned</span> TNumParams&gt;</div><div class="line"><span class="keyword">class </span>myForest : <span class="keyword">public</span> randomForestBase&lt;myForest&lt;TNumParams&gt;,myLabel,myNodeDist,myOutputDist,TNumParams&gt;</div><div class="line">{</div><div class="line">    <span class="keyword">public</span>:</div><div class="line">        <span class="comment">/* You&#39;ll probably want to define a custom constructor here, plus any</span></div><div class="line"><span class="comment">        other public methods */</span></div><div class="line"></div><div class="line">    <span class="keyword">protected</span>:</div><div class="line"></div><div class="line">        <span class="keyword">typedef</span> <span class="keyword">typename</span> randomForestBase&lt;myForest&lt;TNumParams&gt;,myLabel,myNodeDist,myOutputDist,TNumParams&gt;::scoreInternalIndexStruct scoreInternalIndexStruct;</div><div class="line"></div><div class="line"></div><div class="line">        <span class="keywordtype">void</span> printHeaderDescription(std::ofstream &amp;stream)<span class="keyword"> const</span></div><div class="line"><span class="keyword">        </span>{</div><div class="line">            <span class="comment">/* Print a human-readable description of the contents of the header</span></div><div class="line"><span class="comment">            data. Anything printed here is ignord by the library */</span></div><div class="line">        }</div><div class="line"></div><div class="line">        <span class="keywordtype">void</span> printHeaderData(std::ofstream &amp;stream)<span class="keyword"> const</span></div><div class="line"><span class="keyword">        </span>{</div><div class="line">            <span class="comment">/* Print a single line containing any parameters that must be stored</span></div><div class="line"><span class="comment">            in order to reconstruct the model (such as number of classes etc) */</span></div><div class="line">        }</div><div class="line"></div><div class="line">        <span class="keywordtype">void</span> readHeader(std::ifstream &amp;stream)</div><div class="line">        {</div><div class="line">            <span class="comment">/* Read in the data printed using printHeaderData() in order to</span></div><div class="line"><span class="comment">            reconstruct a stored forest model from file */</span></div><div class="line">        }</div><div class="line"></div><div class="line">        <span class="keywordtype">void</span> initialiseNodeDist(<span class="keyword">const</span> <span class="keywordtype">int</span> t, <span class="keyword">const</span> <span class="keywordtype">int</span> n)</div><div class="line">        {</div><div class="line">            <span class="comment">/* Initialise a node distribution before fitting it during training.</span></div><div class="line"><span class="comment">            This can be used to perform any arbitrary action on the node distribution</span></div><div class="line"><span class="comment">            in this-&gt;forest[t].nodes[n].post[0] to prepare for fitting, such as</span></div><div class="line"><span class="comment">            initialising it with certain parameters and/or calling a custom</span></div><div class="line"><span class="comment">            constructor */</span></div><div class="line">        }</div><div class="line"></div><div class="line">        <span class="keywordtype">float</span> minInfoGain(<span class="keyword">const</span> <span class="keywordtype">int</span> tree, <span class="keyword">const</span> <span class="keywordtype">int</span> node)<span class="keyword"> const</span></div><div class="line"><span class="keyword">        </span>{</div><div class="line">            <span class="comment">/* Return the value of information gain threshold for this node</span></div><div class="line"><span class="comment">            during training.</span></div><div class="line"><span class="comment"></span></div><div class="line"><span class="comment">            If the actual information gain from the best split is below this,</span></div><div class="line"><span class="comment">            the node will become a leaf node. This can be give different</span></div><div class="line"><span class="comment">            behaviour in different nodes in the forest if desired, or can simply</span></div><div class="line"><span class="comment">            return a constant. */</span></div><div class="line">        }</div><div class="line"></div><div class="line">        <span class="keyword">template</span> &lt;<span class="keyword">class</span> TLabelIterator&gt;</div><div class="line">        <span class="keyword">template</span> &lt;<span class="keyword">class</span> TLabelIterator, <span class="keyword">class</span> TIdIterator&gt;</div><div class="line">        <span class="keywordtype">void</span> trainingPrecalculations(<span class="keyword">const</span> TLabelIterator first_label, <span class="keyword">const</span> TLabelIterator last_label, <span class="keyword">const</span> TIdIterator first_id)</div><div class="line">        {</div><div class="line">            <span class="comment">/* This is called once at the start of the training routine and may</span></div><div class="line"><span class="comment">            be used to prepare for training on the supplied dataset, for example</span></div><div class="line"><span class="comment">            by precalculating values to speed up subsequent processes */</span></div><div class="line">        }</div><div class="line"></div><div class="line">        <span class="keywordtype">void</span> cleanupPrecalculations()</div><div class="line">        {</div><div class="line">            <span class="comment">/* This is called once at the end of the training routine and may be</span></div><div class="line"><span class="comment">            used, for example, to clear up any data no longer needed */</span></div><div class="line">        }</div><div class="line"></div><div class="line">        <span class="keywordtype">float</span> singleNodeImpurity(<span class="keyword">const</span> TLabelIterator first_label, <span class="keyword">const</span> std::vector&lt;int&gt;&amp; nodebag, <span class="keyword">const</span> <span class="keywordtype">int</span> tree, <span class="keyword">const</span> <span class="keywordtype">int</span> node)<span class="keyword"> const</span></div><div class="line"><span class="keyword">        </span>{</div><div class="line">            <span class="comment">/* Calculate the impurity of the labels in a given node before</span></div><div class="line"><span class="comment">            splitting in a given tree and node. This is used to compare to the</span></div><div class="line"><span class="comment">            value after splitting (found with bestSplit) in order to determine</span></div><div class="line"><span class="comment">            information gain. The labels are accessed via first_label[nodebag[0]],</span></div><div class="line"><span class="comment">            first_label[nodebag[1]] etc */</span></div><div class="line">        }</div><div class="line"></div><div class="line">        <span class="keyword">template</span> &lt;<span class="keyword">class</span> TLabelIterator&gt;</div><div class="line">        <span class="keywordtype">void</span> bestSplit(<span class="keyword">const</span> std::vector&lt;scoreInternalIndexStruct&gt; &amp;data_structs, <span class="keyword">const</span> TLabelIterator first_label, <span class="keyword">const</span> <span class="keywordtype">int</span> tree, <span class="keyword">const</span> <span class="keywordtype">int</span> node, <span class="keyword">const</span> <span class="keywordtype">float</span> initial_impurity,<span class="keywordtype">float</span>&amp; info_gain, <span class="keywordtype">float</span>&amp; thresh)<span class="keyword"> const</span></div><div class="line"><span class="keyword">        </span>{</div><div class="line">            <span class="comment">/* This the key function in the training routine. It takes a list</span></div><div class="line"><span class="comment">            of labels in data_structs which contains an integer-valued internal</span></div><div class="line"><span class="comment">            index (.id) and float-valued feature score (.score) according to the</span></div><div class="line"><span class="comment">            feature functor with the chosen parameters. The elements of</span></div><div class="line"><span class="comment">            data_structs are sorted by increasing values of the score before being</span></div><div class="line"><span class="comment">            passed to this method.</span></div><div class="line"><span class="comment"></span></div><div class="line"><span class="comment">            The labels of each of the training samples in the node can be accessed</span></div><div class="line"><span class="comment">            via first_label[data_structs[0].id], first_label[data_structs[1].id]</span></div><div class="line"><span class="comment">            etc</span></div><div class="line"><span class="comment"></span></div><div class="line"><span class="comment">            This method should find the best way to split the training samples</span></div><div class="line"><span class="comment">            with a single score threshold. The &#39;best&#39; split is calculated with</span></div><div class="line"><span class="comment">            regards to the labels of the training samples, and is left to the</span></div><div class="line"><span class="comment">            user to define.</span></div><div class="line"><span class="comment"></span></div><div class="line"><span class="comment">            The method returns (by reference) the chosen threshold (thresh) and</span></div><div class="line"><span class="comment">            the resulting information gain between the initial impurity before</span></div><div class="line"><span class="comment">            splitting (which is passed in as initial_impurity in order to avoid</span></div><div class="line"><span class="comment">            redundant repeated calculations), and the impurity after splitting.</span></div><div class="line"><span class="comment">            */</span></div><div class="line">        }</div><div class="line"></div><div class="line">        <span class="comment">/* Other data etc */</span></div><div class="line">};</div></div><!-- fragment --><p>Note that your class should inherit from the <a class="el" href="classcanopy_1_1random_forest_base.html" title="Base class for random forests models from which all specific models are derived using CRTP...">canopy::randomForestBase</a> class, and therefore provide the 5 template parameters of the base class. The 2nd, 3rd, and 4th template parameters are respectively the label type, the node distribution type and the output distribution type, which need to match the relevant types of your node and output distribution models.</p>
<p>The first template parameter must be the type of your own forest model (being declared). This is in order to implement the <a href="http://en.m.wikipedia.org/wiki/Curiously_recurring_template_pattern">CRTP</a> idiom of static polymorphism.</p>
<p>The final template parameter is the number of parameters of the feature functor, and you will often want to make this a template parameter of your model also (as in the example) to allow for maximum flexibility.</p>
<p>The most complicated task in defining your own model is defining the training procedure, which is controlled by the implementations of the <code>trainingPrecalculations</code>, <code>cleanupPrecalculations</code>, <code>singleNodeImpurity</code>, <code>bestSplit</code>, and <code>minInfoGain</code> methods.</p>
<p>These methods are called by the base class's training procedure as follows:</p>
<ul>
<li>The <code>trainingPrecalculations</code> method is called on the entire training dataset in order to perform any necessary pre-calculations.</li>
<li>Then the training continues on a node-by-node basis, starting at the root of each tree (trees are trained independently and in parallel). Each node receives a list of training IDs from its parent node. It then calculates the 'impurity' of that set before splitting with the <code>singleNodeImpurity</code> method (in a sense defined by you).</li>
<li>Next, a number of random generated feature functor parameter sets are generated and applied to the IDs in the node. Each parameter set results in a feature score value for each training ID. These are then sorted and passed to the <code>bestSplit</code> method, which calculates the best (in a sense defined by you) feature score threshold to use to split the data into two child nodes using this parameter set.</li>
<li>The parameter set resulting in the best information gain (as returned by the <code>bestSplit</code> method), is chosen as the feature set for that node.</li>
<li>However if the best information gain is lower than the value returned by the <code>minInfoGain</code> method, the node is declared as a leaf node. Training also stops when the maximum depth is reached or the number of training data in the node falls below the user-supplied threshold.</li>
<li>After training is complete for all trees, the <code>cleanupPrecalculations</code> method is called.</li>
</ul>
<p>The node distribution's <code>fit</code> method is also called as necessary to fit a node distrbution to the training dataset. </p>
</div></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.11
</small></address>
</body>
</html>
