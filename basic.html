<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.11"/>
<title>Canopy: Overview and Basic Usage</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { init_search(); });
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Canopy
   &#160;<span id="projectnumber">1.0</span>
   </div>
   <div id="projectbrief">The header-only random forests library</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.11 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li class="current"><a href="pages.html"><span>Related&#160;Pages</span></a></li>
      <li><a href="namespaces.html"><span>Namespaces</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="index.html">Overview</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Overview and Basic Usage </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#forests">Random Forests</a></li>
<li class="level1"><a href="#randomForestBase">The randomForestBase Class</a></li>
<li class="level1"><a href="#functors">Defining Feature Functors</a><ul><li class="level2"><a href="#single">Single Feature Functors</a></li>
<li class="level2"><a href="#groupwise">Groupwise Feature Functors</a></li>
<li class="level2"><a href="#thread_safety">Thread Safety of Feature Functors</a></li>
<li class="level2"><a href="#lambdas">Using Lambdas As Feature Functors</a></li>
</ul>
</li>
<li class="level1"><a href="#training">Training A Forest Model</a><ul><li class="level2"><a href="#params">Parameter Generator Functors</a></li>
<li class="level2"><a href="#train_method">Using The Train Method</a></li>
</ul>
</li>
<li class="level1"><a href="#distribution_prediction">Distribution Prediction</a></li>
<li class="level1"><a href="#probability_evaluation">Probability Evaluation</a></li>
<li class="level1"><a href="#example">A Full Example</a></li>
</ul>
</div>
<div class="textblock"><h1><a class="anchor" id="forests"></a>
Random Forests</h1>
<p>In order to describe how the canopy library works, we will first need an abstract description of the random forests algorithm. I recommend the following as an excellent thorough introduction for those who need to learn or revise the basics:</p>
<ul>
<li>A. Criminisi, J. Shotton and E. Konukoglu. <a href="https://www.microsoft.com/en-us/research/publication/decision-forests-for-classification-regression-density-estimation-manifold-learning-and-semi-supervised-learning/">Decision Forests for Classification, Regression, Density Estimation, Manifold Learning and Semi-Supervised Learning</a>. Technical Report. Microsoft Research.</li>
</ul>
<p>A random forest is a collection of binary decision trees that is used to predict some quantity based on some <b>input features</b>. We will refer to the quantity being predicted as a <b>label</b>. The nature of the label will vary with the task being performed. For example, in a classification task, the label wil be a discrete (integer) variable representing the class. For a regression problem, the label will be a continuous quantity (represented by a floating point number), etc. The features can be similarly general, they could be a set of pre-recorded measurements, or the result of some function applied to an image, video, or signal.</p>
<p>A data point is passed into the <b>root node</b> of each tree in the forest, and is passed down the tree until it reaches a <b>leaf node</b>. Each node looks at the values of some of the features in order to decide whether to pass the data point down to its left or right <b>child node</b>. This repeats until the leaf node, which has no children, is reached. The leaf node contains some distribution over the value of the label given the input, we will call such a distribution a <b>node distribution</b>.</p>
<p>There are two tasks that the forest models created using canopy can perform at this point:</p>
<ul>
<li>Combine the <b>node distributions</b> reached in each of the trees to give a single new distribution over the value of the label. We will call this distribution an <b>output distribution</b>. In general its form may be the same as the <b>node distribution</b> or it may be different. We will call this the <b>distribution prediction</b> task.</li>
<li>Evaluate the probability of a certain value of the label variable. This is done by using the <b>node distributions</b> reached in each tree to evaluate the probability of that label, and then averaging this result over all the trees. We will call this the <b>probability evaluation</b> task.</li>
</ul>
<p>In order to train each tree in the forests, a randomly selected set of features are tested to see which can split the labels in the training set to give the <b>purest</b> child nodes. The concrete definition of <b>purity</b> depends on the type of the labels and the particular problem at hand.</p>
<h1><a class="anchor" id="randomForestBase"></a>
The randomForestBase Class</h1>
<p>The canopy library uses a single base class called <a class="el" href="classcanopy_1_1random_forest_base.html" title="Base class for random forests models from which all specific models are derived using CRTP...">canopy::randomForestBase</a> to provide the general framework for training and testing random forests as described in the previous section.</p>
<p>The <code>randomForestBase</code> class looks something like this:</p>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">class</span> TDerived, <span class="keyword">class</span> TLabel, <span class="keyword">class</span> TNodeDist, <span class="keyword">class</span> TOutputDist, <span class="keywordtype">unsigned</span> TNumParams&gt;</div><div class="line"><span class="keyword">class </span>randomForestBase</div><div class="line">{</div><div class="line">    <span class="comment">//...</span></div><div class="line">};</div></div><!-- fragment --><p>There are several template parameters that allow classes derived from this base class to implement a wide range of different behaviours:</p>
<ul>
<li><b>TDerived</b> This is the type of the derived class, needed in order to use the <a href="https://en.wikipedia.org/wiki/Curiously_recurring_template_pattern">CRTP</a> form of static polymorphism.</li>
<li><b>TLabel</b> The type of the <b>label</b> (the variable to be predicted). This can in principle be an arbitrary type.</li>
<li><b>TNodeDist</b> This is the type of the <b>node distribution</b>, which must have a specific form dictated by the choice of <b>TLabel</b>. See <a class="el" href="advanced.html#node_dist">Defining Your Own Node Distribution</a>.</li>
<li><b>TOutputDist</b> This is the type of the <b>output distribution</b>, which must have a specific form dictated by the choice of <b>TNodeDist</b>. See <a class="el" href="advanced.html#output_dist">Defining Your Own Output Distribution</a>.</li>
<li><b>TNumParams</b> This is the number of parameters of the feature callback functor. See <a class="el" href="basic.html#functors">Defining Feature Functors</a>.</li>
</ul>
<p>Specific random forest models may be derived from this base class using <a href="https://en.wikipedia.org/wiki/Curiously_recurring_template_pattern">CRTP</a> with a specific set of these template parameters (although <b>TNumParams</b> is typically left unspecified).</p>
<p>Canopy proivides two pre-defined random forest models:</p>
<ul>
<li><b><a class="el" href="classcanopy_1_1classifier.html" title="Implements a random forest classifier model to predict a discrete output label. ">canopy::classifier</a></b> A random forest classifier to predict a discrete label (<b>TLabel</b> = <code>int</code>) with a <a class="el" href="classcanopy_1_1discrete_distribution.html" title="A distribution that defines the probabilities over a number of discrete (integer-valued) class labels...">canopy::discreteDistribution</a> object serving as the node and output distribution type (<b>TNodeDist</b> = <a class="el" href="classcanopy_1_1discrete_distribution.html" title="A distribution that defines the probabilities over a number of discrete (integer-valued) class labels...">canopy::discreteDistribution</a> and <b>TOutputDist</b> = <a class="el" href="classcanopy_1_1discrete_distribution.html" title="A distribution that defines the probabilities over a number of discrete (integer-valued) class labels...">canopy::discreteDistribution</a>).</li>
<li><b><a class="el" href="classcanopy_1_1circular_regressor.html" title="Implements a random forest classifier model to predict a circular-valued output label. ">canopy::circularRegressor</a></b> A random forest model for predicting a circular-valued (angular) output variable, represented using a floating point number (<b>TLabel</b> = <code>float</code>) and using a <a class="el" href="classcanopy_1_1von_mises_distribution.html" title="A distribution that defines the probabilities over a circular-valued label. ">canopy::vonMisesDistribution</a> object as the node and output distribution type.</li>
</ul>
<p>Furthermore, you can inherit from this base class to <a class="el" href="advanced.html#custom_forest">define your own random forest model</a> for other tasks.</p>
<h1><a class="anchor" id="functors"></a>
Defining Feature Functors</h1>
<p>Canopy handles features during both testing and traning using functor objects in order to allow for maximum flexibility for the feature calculation process to to execute arbitrary user code and access arbitrary user data.</p>
<p>There are two different forms of feature functor that are accepted by canopy under different circumstances. The difference between these two forms is how they handle multiple test data.</p>
<p>In both cases, the purpose of the functor object is to take some test points identified by some <b>"ID"</b> type, apply some function to them specified by some number of integer-valued parameters, and return a single floating-point value for each input ID as the result.</p>
<p>The <b>ID</b> can be any arbitrary data type used to identify a test data point. The ID variables are not used or moved around by Canopy methods, so it really doesn't matter what they are as long as your own functors can make sense of them. In the simplest case (as in the <a class="el" href="basic.html#example">provided example</a>), the ID is simply an integer representing the index of the test sample in some list. However, it could be more complicated than this, for example it could be a 2D vector representing an image location in an image, or a 3D vector representing a 3D location within a volume image.</p>
<p>The function that the functor applies to each test data point can be parameterised by an arbitrary number of integer-valued parameters, which are passed to the functor by const reference in a std::array. The number of parameters is controlled by the <b>TNumParams</b> template parameter of the base class. Again it is up to you to define the meaning of these parameters, the Canopy code just passes them around without using them.</p>
<h2><a class="anchor" id="single"></a>
Single Feature Functors</h2>
<p>The first of the two forms calculates the features for test data in a one-by-one fashion. We will refer to this form as the <b>Single Feature Functor</b> form. This is the simplest option and involves the smallest overhead.</p>
<p>A <b>Single Feature Functor</b> to be used with a forest with <b>TNumParams</b> = <code>MyNumParams</code> and using ID type <code>MyIDType</code> must look like this:</p>
<div class="fragment"><div class="line"><span class="keyword">struct </span>MySingleFeatureFunctor</div><div class="line">{</div><div class="line">    <span class="keywordtype">float</span> operator() (<span class="keyword">const</span> MyIDType <span class="keywordtype">id</span>, <span class="keyword">const</span> std::array&lt;int,MyNumParams&gt;&amp; params)</div><div class="line">    {</div><div class="line">        <span class="comment">// Add your code here to apply the feature calculation process to &#39;id&#39; using the parameters in &#39;params&#39;</span></div><div class="line">        <span class="comment">// and return the result</span></div><div class="line">    }</div><div class="line"></div><div class="line">    <span class="comment">// Other user defined data and methods can go here if you want</span></div><div class="line">};</div></div><!-- fragment --><h2><a class="anchor" id="groupwise"></a>
Groupwise Feature Functors</h2>
<p>The second form allows for calculating the features for multiple test points together. We will refer to this as the <b>Groupwise Feature Functor</b> form. This can be advantageous when such a process can be made quicker than evaluating the test data individually. For example, if the test data are different image locations in the same image, then it may be faster the perform some process (e.g. an FFT) on the entire image than work on each point individually.</p>
<p>This form will be referred to as the <b>Groupwise Feature Functor</b>. In contrast to the <b>Single Feature Functor</b>, which receives a single ID variable directly, the <b>Groupwise Feature Functor</b> receives multiple IDs via iterators. Because canopy uses some moderately complex iterator hackery internally, working out the type of the iterator it will pass your functor can be a little complicated. Therefore, I strongly recommend defining a method template and letting the compiler do the type inference for you. Taking this approach, a <b>Groupwise Feature Functor</b> for use with a forest with <b>TNumParams</b> = <code>MyNumParams</code> should look like this:</p>
<div class="fragment"><div class="line"><span class="keyword">struct </span>MyGroupwiseFeatureFunctor</div><div class="line">{</div><div class="line">    <span class="comment">// TIdIterator will be deduced by the compiler for you</span></div><div class="line">    <span class="keyword">template</span>&lt;<span class="keyword">class</span> TIdIterator&gt;</div><div class="line">    <span class="keywordtype">void</span> operator() (TIdIterator first_id, <span class="keyword">const</span> TIdIterator last_id, <span class="keyword">const</span> std::array&lt;int,MyNumParams&gt;&amp; params, std::vector&lt;float&gt;::iterator out_it)</div><div class="line">    {</div><div class="line">        <span class="comment">// Add your code here to iterate thhrough the elements between first_id and last_id and apply the feature calculation process to each id using</span></div><div class="line">        <span class="comment">// the parameters in &#39;params&#39; and place the result in the corresponding location in &#39;out_it&#39;</span></div><div class="line"></div><div class="line">        <span class="comment">// You may assume that TIdIterator is a random access iterator type and dereferences to your ID type</span></div><div class="line">    }</div><div class="line"></div><div class="line">    <span class="comment">// Other user defined data and methods can go here if you want</span></div><div class="line">};</div></div><!-- fragment --><p>If you unsure about what a random access iterator is and how to use it, see <a href="http://en.cppreference.com/w/cpp/iterator">here</a>.</p>
<p>Note also that it is entirely possible to define an object that can serve as either a <b>Single Feature Functor</b> or <b>Groupwise Feature Functor</b> by overloading the <code>operator()</code> for both cases.</p>
<h2><a class="anchor" id="thread_safety"></a>
Thread Safety of Feature Functors</h2>
<p>Because canopy uses OpenMP threads to query the different trees in a forest in parallel, the feature functors will be used simultaneously by multiple threads. It is your responsililty to ensure that the functors are thread-safe, i.e. there are no data races that can be caused due to access from multiple threads. If there are potential data races, you should use OpenMP lock variables to prevent them.</p>
<p>Alternatively you can run with a single thread by compiling without OpenMP (remove the <code>-fopenmp</code> directive in g++). This will give you some compiler warnings about unknown pragmas, but otherwise the code will compile and run fine, but usually execute slower than with multiple threads.</p>
<h2><a class="anchor" id="lambdas"></a>
Using Lambdas As Feature Functors</h2>
<p>So far we have considered creating a custom <code>struct</code> (or <code>class</code>) to act as our functor. This is attractive as it allows you to encapsulate the feature extraction process along with all its data in a single object.</p>
<p>There is another option though, the use of a C++11 <a href="http://www.cprogramming.com/c++11/c++11-lambda-closures.html">lambda</a>. This approach is attractive because it requires less "boilerplate" code and the lambda can easily access data defined elsewhere in your programme via lambda captures. Defining a <b>Single Feature Functor</b> for a forest model with <b>TNumParams</b> = <code>MyNumParams</code> and using ID type <code>MyIDType</code> using a lambda would look like this:</p>
<div class="fragment"><div class="line"><span class="keyword">auto</span> MySingleFeatureLambda = [] (<span class="keyword">const</span> MyIDType id, <span class="keyword">const</span> std::array&lt;int,MyNumParams&gt;&amp; params)</div><div class="line">{</div><div class="line">    <span class="comment">// Add your code here to apply the feature calculation process to &#39;id&#39; using the parameters in &#39;params&#39;</span></div><div class="line">    <span class="comment">// and return the result</span></div><div class="line">};</div></div><!-- fragment --><p>However, extending this to the <b>Groupwise Feature Functor</b> causes some headaches because of those tricky iterator types. With C++11, you would need to manually work out all those types. Luckily however, the C++14 standard fixes this problem by allowing you to define generic lambdas using the <code>auto</code> keyword in place of the types. Then the compiler will deduce the types for you based on how the lambda is called from within the canopy code. Again assuming we are working with a forest model with <b>TNumParams</b> = <code>MyNumParams</code> and using ID type <code>MyIDType</code>, a <b>Groupwise Feature Functor</b> implemented using a lambda will look like this:</p>
<div class="fragment"><div class="line"><span class="keyword">auto</span> MyGroupwiseFeatureLambda = [] (<span class="keyword">auto</span> first_id, <span class="keyword">const</span> <span class="keyword">auto</span> last_id, <span class="keyword">const</span> std::array&lt;int,MyNumParams&gt;&amp; params, std::vector&lt;float&gt;::iterator out_it)</div><div class="line">{</div><div class="line">    <span class="comment">// Add your code here to iterate thhrough the elements between first_id and last_id and apply the feature calculation process to each id using</span></div><div class="line">    <span class="comment">// the parameters in &#39;params&#39; and place the result in the corresponding location in &#39;out_it&#39;</span></div><div class="line"></div><div class="line">    <span class="comment">// You may assume that the type of first_id and last_id is a random access iterator type and dereferences to your ID type</span></div><div class="line">};</div></div><!-- fragment --><p>The <a class="el" href="basic.html#example">example</a> provides an example of using a lambda as a feature functor that captures local data.</p>
<h1><a class="anchor" id="training"></a>
Training A Forest Model</h1>
<p>In order to train a model you must create a <b>Groupwise Feature Functor</b> object and pass it to the randomForestBase's train() method.</p>
<h2><a class="anchor" id="params"></a>
Parameter Generator Functors</h2>
<p>Additionally, you must define a second functor object that generates valid combinations of parameters for your split functors on demand. Which combinations are valid depends entirely on the feature calculation process that you are using.</p>
<p>The <b>Parameter Generator Functor</b> for a forest model with <b>TNumParams</b> = <code>MyNumParams</code> must take the following form:</p>
<div class="fragment"><div class="line"><span class="keyword">struct </span>MyParameterGeneratorFunctor</div><div class="line">{</div><div class="line">    <span class="keywordtype">void</span> operator() (std::array&lt;int,MyNumParams&gt;&amp; params)</div><div class="line">    {</div><div class="line">        <span class="comment">// Generate a random valid combination of parameters and store in params</span></div><div class="line">    }</div><div class="line"></div><div class="line">    <span class="comment">// Other user defined data and methods can go here if you want</span></div><div class="line">};</div></div><!-- fragment --><p>Alternatively you could use a lambda, as follows:</p>
<div class="fragment"><div class="line"><span class="keyword">auto</span> MyParameterGeneratorLambda = [] (std::array&lt;int,MyNumParams&gt;&amp; params)</div><div class="line">{</div><div class="line">    <span class="comment">// Generate a random valid combination of parameters and store in params</span></div><div class="line">};</div></div><!-- fragment --><p>A third option, if your parameter generation problem is straightforward, is to use the provided <a class="el" href="classcanopy_1_1default_parameter_generator.html" title="A simple parameter generator functor for training forest models. ">canopy::defaultParameterGenerator</a> class. This can handle cases where the parameters can be generated independently from a uniform distribution over the integers between 0 and some user-defined upper limit.</p>
<h2><a class="anchor" id="train_method"></a>
Using The Train Method</h2>
<p>Once you have created your feature functor and parameter generation functor, and list of IDs of the data points in the training set, you are ready to train the model. You do this by creating a forest model object and calling the <a class="el" href="classcanopy_1_1random_forest_base.html#adc82fbe49123558dd5ee597f005cb16b" title="Train the random forest model on training data. ">canopy::randomForestBase::train()</a> method.</p>
<p>As an example, for a <a class="el" href="classcanopy_1_1classifier.html" title="Implements a random forest classifier model to predict a discrete output label. ">canopy::classifier</a> model, this looks something like this:</p>
<div class="fragment"><div class="line"><a class="code" href="classcanopy_1_1classifier.html">canopy::classifier&lt;MyNumParams&gt;</a> my_classifier(my_num_classes,my_num_trees,my_num_levels);</div><div class="line">my_classifer.train(my_first_training_id,my_last_training_id,my_first_label,my_feature_functor,my_param_functor,my_num_param_trials);</div></div><!-- fragment --><p><code>my_num_classes</code> defines the number of classes in the classfication function. <code>my_num_trees</code> and <code>my_num_levels</code> respectively define the number of trees in the forest model, and the number of levels in each tree. The training set is defined by a pair of iterators to ID variables (<code>my_first_training_id</code> and <code>my_last_training_id</code>) along with the corresponding values of the label (my_first_label). The <code>my_num_param_trials</code> controls how many features are tested in each split node.</p>
<p>Once a model has been trained, it can be stored in a file for later use using the <a class="el" href="classcanopy_1_1random_forest_base.html#a677e06d4acfdb2e0fdffc8e2971a66df" title="Write a trained model to a .tr file to be stored and re-used. ">canopy::randomForestBase::writeToFile()</a> method.</p>
<h1><a class="anchor" id="distribution_prediction"></a>
Distribution Prediction</h1>
<p>Recall that the distribution prediction task is that of predicting an <b>output</b> <b>distribution</b> over the label variable by combining the <b>node</b> <b>distributions</b> reached in each of the trees.</p>
<p>To do this we first need to create an <b>output distribution</b> object for each of the data points. We can then pass these objects to the forest model and have it update the parameters based on the features. The type of the <b>output distribution</b> depends on the specific forest model being used. For a <a class="el" href="classcanopy_1_1classifier.html" title="Implements a random forest classifier model to predict a discrete output label. ">canopy::classifier</a> model, it is a <a class="el" href="classcanopy_1_1discrete_distribution.html" title="A distribution that defines the probabilities over a number of discrete (integer-valued) class labels...">canopy::discreteDistribution</a>, and for a <a class="el" href="classcanopy_1_1circular_regressor.html" title="Implements a random forest classifier model to predict a circular-valued output label. ">canopy::circularRegressor</a> it is a <a class="el" href="classcanopy_1_1von_mises_distribution.html" title="A distribution that defines the probabilities over a circular-valued label. ">canopy::vonMisesDistribution</a>.</p>
<p>There are two methods for distribution prediction. The first, <a class="el" href="classcanopy_1_1random_forest_base.html#a99ba77f1b521fc6992a8841294caedd1" title="Predict the output distribution for a number of IDs. ">canopy::randomForestBase::predictDistSingle()</a> uses a <b>Single Feature Functor</b> to calculate the features. The second <a class="el" href="classcanopy_1_1random_forest_base.html#a1863eb32b1ae355c52c53669dd543593" title="Predict the output distribution for a number of IDs. ">canopy::randomForestBase::predictDistGroupwise()</a> performs the same function but uses a <b>Groupwise Feature Functor</b> instead.</p>
<p>Doing this for a <a class="el" href="classcanopy_1_1classifier.html" title="Implements a random forest classifier model to predict a discrete output label. ">canopy::classifier</a> looks something like this:</p>
<div class="fragment"><div class="line"><span class="comment">// Create array of output distributions</span></div><div class="line">std::vector&lt;canopy::discreteDistribution&gt; dists(num_test_data_points);</div><div class="line"></div><div class="line"><span class="comment">// Initialise them, which in this case involves specifying the number of classes</span></div><div class="line"><span class="keywordflow">for</span>(<span class="keyword">auto</span> &amp; d : dists)</div><div class="line">    d.initialise(num_classes);</div><div class="line"></div><div class="line"><span class="comment">// Use the methods to find the parameters of the distributions</span></div><div class="line">my_classifier.predictDistSingle(my_first_test_id,my_last_test_id,dists.begin(),my_single_feature_functor);</div><div class="line"><span class="comment">// Or...</span></div><div class="line">my_classifier.predictDistGroupwise(my_first_test_id,my_last_test_id,dists.begin(),my_groupwise_feature_functor);</div><div class="line"></div><div class="line"><span class="comment">// Now the forest has found the distribution&#39;s parameters, we can use them for whatever</span></div><div class="line">std::cout &lt;&lt; dists[0].pdf(1) &lt;&lt; std::endl; <span class="comment">// output probability that ID 0 has class label 1</span></div></div><!-- fragment --><h1><a class="anchor" id="probability_evaluation"></a>
Probability Evaluation</h1>
<p>Recall that <b>probability evaluation</b> is the task of evaluating the probability of certain label, according to the forest. It is worth briefly considering how this is different to simply performing the <b>distribution prediction</b> task, and then just evaluating the probability of the label according to the output distribution (which is what we did in the final line of the above snippet). The <b>probability evaluation</b> task evaluates the probability of the label under each <b>node distribution</b> and then averages the result, whereas the <b>distribution prediction</b> combines several <b>node distributions</b> and then finds the probability according to this combined distribution.</p>
<p>Where it is possible to find a method of combining the distributions in a way that does not result in loss of information, the result is exactly the same. This is the case with the discrete distribution. However it is <b>not</b> the case with the von Mises distribution in the cirular regression task. In the latter case, the <b>distribution prediction</b> procedure combines several von Mises distributions to create a further von Mises distribution using a sensor-fusion approach that results in loss of information. This is easy to see, because a uni-modal von Mises distribution cannot possibly fully represent the combination of several other distributions that each have different means. Consequently you will get a different answer for probability of a certain label if you use the direct <b>probability evaluation</b> approach compared to if you use the <b>distribution prediction</b>.</p>
<p>Therefore, in general, if you want to evaluate the probabilty of a certain label you should use the direct <b>probability evaluation</b> approach. The <b>distribution prediction</b> task is more useful if you want some summary statistics, such as a point estimate of the mean or variance.</p>
<p>With that said, let's look at how to perform <b>probability evaluation</b>. Like in the <b>distribution prediction</b>, there are both single and groupwise versions that are otherwise equivalent (<a class="el" href="classcanopy_1_1random_forest_base.html#a73019f5709e606a43c5898abb4db60fb" title="Evaluate the probability of a certain value of the label for a set of data points. ">canopy::randomForestBase::probabilitySingle()</a> and <a class="el" href="classcanopy_1_1random_forest_base.html#ab5afcf70c2ce8271748f9a8fa16e8b88" title="Evaluate the probability of a certain value of the label for a set of data points. ">canopy::randomForestBase::probabilityGroupwise()</a>). Now however, we don't need to pass in any distribution, but we do need the method some container of floats in which to place the results, and the values of the labels for which it is to evaluate the probability.</p>
<div class="fragment"><div class="line"><span class="comment">// Container to hold the results</span></div><div class="line">std::vector&lt;float&gt; results(num_test_ids);</div><div class="line">the_classifier.probabilitySingle(my_first_test_id,my_last_test_id, my_first_label, results.begin(), <span class="keyword">false</span>, my_single_feature_functor);</div><div class="line"><span class="comment">// Or...</span></div><div class="line">the_classifier.probabilityGroupwise(my_first_test_id,my_last_test_id, my_first_label, results.begin(), <span class="keyword">false</span>, my_groupwise_feature_functor);</div></div><!-- fragment --><p>These methods can either evaluate the probability of a single label for multiple test IDs, or each test ID can have its own value of the label, depending on the value of the 5th parameter.</p>
<h1><a class="anchor" id="example"></a>
A Full Example</h1>
<p>This example file may be found in the <code>examples</code> directory of the canopy repository along with a <code>Makefile</code> that can be used to compile it using GNU make and g++.</p>
<p>It implements a toy example using a random forest classifier to classify a number of data points into different classes. The features are randomly generated from a Gaussian distribution with a different, randomly-generated mean and variance for each class and pre-stored in an array.</p>
<div class="fragment"><div class="line"><span class="comment">// Standard Library Headers</span></div><div class="line"><span class="preprocessor">#include &lt;iostream&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;array&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;random&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;algorithm&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;string&gt;</span></div><div class="line"></div><div class="line"><span class="comment">// The canopy classifier header</span></div><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="classifier_8hpp.html">canopy/classifier/classifier.hpp</a>&gt;</span></div><div class="line"></div><div class="line"><span class="comment">/* This programme demonstrates how to use the basic functionality of the canopy</span></div><div class="line"><span class="comment">library. It trains a random forest classifier to proedict the discrete label</span></div><div class="line"><span class="comment">of test data given some features. The features are randomly generated from</span></div><div class="line"><span class="comment">a Gaussian distribution with different, randomly-chosen mean and variance</span></div><div class="line"><span class="comment">parameters for each of the discrete classes.</span></div><div class="line"><span class="comment">*/</span></div><div class="line"></div><div class="line"><span class="keywordtype">int</span> main()</div><div class="line">{</div><div class="line">    <span class="comment">/* Parameters of the test */</span></div><div class="line">    constexpr <span class="keywordtype">unsigned</span> N_CLASSES = 3; <span class="comment">// number of discrete class labels</span></div><div class="line">    constexpr <span class="keywordtype">unsigned</span> TRAINING_DATA_PER_PER_CLASS = 200;</div><div class="line">    constexpr <span class="keywordtype">unsigned</span> TOTAL_TRAINING_DATA = N_CLASSES * TRAINING_DATA_PER_PER_CLASS;</div><div class="line">    constexpr <span class="keywordtype">unsigned</span> N_DIMS = 2; <span class="comment">// dimensionality of the feature space</span></div><div class="line">    constexpr <span class="keywordtype">double</span> MIN_MU = 0.0; <span class="comment">// range of the randomly-generated mean parameters (min)</span></div><div class="line">    constexpr <span class="keywordtype">double</span> MAX_MU = 10.0; <span class="comment">// range of the randomly-generated mean parameters (max)</span></div><div class="line">    constexpr <span class="keywordtype">double</span> MAX_SIGMA = 3.0; <span class="comment">// maximum value for randomly-generated standard deviation parameters</span></div><div class="line">    constexpr <span class="keywordtype">int</span> N_TREES = 128; <span class="comment">//number of trees in the random forest</span></div><div class="line">    constexpr <span class="keywordtype">int</span> N_LEVELS = 10; <span class="comment">// maximum number of levels in each tree</span></div><div class="line">    constexpr <span class="keywordtype">unsigned</span> N_TESTS = 10; <span class="comment">//number of test data</span></div><div class="line">    <span class="keyword">const</span> std::string FILENAME = <span class="stringliteral">&quot;example_model.tr&quot;</span>; <span class="comment">// file to save the model in</span></div><div class="line"></div><div class="line">    <span class="comment">/* Set up random number generation */</span></div><div class="line">    std::default_random_engine rand_engine;</div><div class="line">    std::random_device rd{};</div><div class="line">    rand_engine.seed(rd());</div><div class="line">    std::normal_distribution&lt;double&gt; norm_dist;</div><div class="line">    std::uniform_int_distribution&lt;int&gt; uni_int_dist;</div><div class="line">    std::uniform_real_distribution&lt;double&gt; uni_real_dist;</div><div class="line"></div><div class="line">    <span class="comment">/* Randomly generate sigma and mu parameters for each class assuming</span></div><div class="line"><span class="comment">    axis-aligned distributions for simplicity. These are arrays with classes</span></div><div class="line"><span class="comment">    down the first dimension and the feature space dimension down the second */</span></div><div class="line">    std::array&lt;std::array&lt;double,N_DIMS&gt;,N_CLASSES&gt; mu;</div><div class="line">    std::array&lt;std::array&lt;double,N_DIMS&gt;,N_CLASSES&gt; sigma;</div><div class="line">    <span class="keywordflow">for</span>(<span class="keywordtype">unsigned</span> c = 0; c &lt; N_CLASSES; ++c)</div><div class="line">    {</div><div class="line">        <span class="keywordflow">for</span>(<span class="keywordtype">unsigned</span> d = 0; d &lt; N_DIMS; ++d)</div><div class="line">        {</div><div class="line">            mu[c][d] = uni_real_dist(rand_engine,std::uniform_real_distribution&lt;double&gt;::param_type{MIN_MU,MAX_MU});</div><div class="line">            sigma[c][d] = uni_real_dist(rand_engine,std::uniform_real_distribution&lt;double&gt;::param_type{0.0,MAX_SIGMA});</div><div class="line">        }</div><div class="line">    }</div><div class="line"></div><div class="line">    <span class="comment">/* Generate training data using these distributions */</span></div><div class="line">    std::array&lt;std::array&lt;double,N_DIMS&gt;,TOTAL_TRAINING_DATA&gt; training_data_features;</div><div class="line">    std::array&lt;int,TOTAL_TRAINING_DATA&gt; training_data_labels;</div><div class="line">    <span class="keywordflow">for</span>(<span class="keywordtype">unsigned</span> c = 0; c &lt; N_CLASSES; ++c)</div><div class="line">    {</div><div class="line">        <span class="keywordflow">for</span>(<span class="keywordtype">unsigned</span> n = 0; n &lt; TRAINING_DATA_PER_PER_CLASS; ++n)</div><div class="line">        {</div><div class="line">            <span class="keyword">const</span> <span class="keywordtype">unsigned</span> i = c*TRAINING_DATA_PER_PER_CLASS + n;</div><div class="line">            training_data_labels[i] = c;</div><div class="line"></div><div class="line">            <span class="keywordflow">for</span>(<span class="keywordtype">unsigned</span> d = 0; d &lt; N_DIMS; ++d)</div><div class="line">            {</div><div class="line">                training_data_features[i][d] = norm_dist(rand_engine,std::normal_distribution&lt;double&gt;::param_type{mu[c][d],sigma[c][d]});</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line"></div><div class="line">    <span class="comment">/* Create a classifer object and initialise with the number of classes, trees</span></div><div class="line"><span class="comment">    and levels. The TNumParams template parameter is one because there is a single</span></div><div class="line"><span class="comment">    parameter of the feature calculation process, which indexes the different features</span></div><div class="line"><span class="comment">    in the list */</span></div><div class="line">    <a class="code" href="classcanopy_1_1classifier.html">canopy::classifier&lt;1&gt;</a> the_classifier(N_CLASSES,N_TREES,N_LEVELS);</div><div class="line"></div><div class="line">    <span class="comment">/* Create a groupwise feature functor object in order to train the model.</span></div><div class="line"><span class="comment">    A C++14 generic lambda is a convenient way to do this</span></div><div class="line"><span class="comment">    as it can capture the data array by reference and figure out all the types</span></div><div class="line"><span class="comment">    for us */</span></div><div class="line">    <span class="keyword">auto</span> train_feature_lambda = [&amp;] (<span class="keyword">auto</span> first_id, <span class="keyword">const</span> <span class="keyword">auto</span> last_id, <span class="keyword">const</span> std::array&lt;int,1&gt;&amp; params, std::vector&lt;float&gt;::iterator out_it)</div><div class="line">    {</div><div class="line">        <span class="comment">/* Iterate over the IDs */</span></div><div class="line">        <span class="keywordflow">while</span>(first_id != last_id)</div><div class="line">        {</div><div class="line">            <span class="comment">/* ID for this data point found by dereferencing iterator */</span></div><div class="line">            <span class="keyword">const</span> <span class="keywordtype">int</span> <span class="keywordtype">id</span> = *first_id;</div><div class="line"></div><div class="line">            <span class="comment">/* The first and only parameter represents the dimension of the</span></div><div class="line"><span class="comment">            feature space to use */</span></div><div class="line">            <span class="keyword">const</span> <span class="keywordtype">int</span> d = params[0];</div><div class="line"></div><div class="line">            <span class="comment">/* Look up the pre-calculated feature value for this ID and dimension</span></div><div class="line"><span class="comment">            and place it in the output iterator</span></div><div class="line"><span class="comment">            (The training_data_features array is captured by reference) */</span></div><div class="line">            *out_it++ = training_data_features[id][d];</div><div class="line"></div><div class="line">            <span class="comment">/* Advance the iterator */</span></div><div class="line">            ++first_id;</div><div class="line">        }</div><div class="line">    };</div><div class="line"></div><div class="line">    <span class="comment">/* Create a parameter generator functor for training, which simply selects a</span></div><div class="line"><span class="comment">    random dimension. We could equally well use a canopy::defaultParameterGenerator</span></div><div class="line"><span class="comment">    here */</span></div><div class="line">    <span class="keyword">auto</span> param_lambda = [&amp;] (std::array&lt;int,1&gt;&amp; params)</div><div class="line">    {</div><div class="line">        params[0] = uni_int_dist(rand_engine,std::uniform_int_distribution&lt;int&gt;::param_type{0,N_DIMS-1});</div><div class="line">    };</div><div class="line"></div><div class="line">    <span class="comment">/* Finally we need a way of identifying each of the data points in the training</span></div><div class="line"><span class="comment">    set. This is done by the index of the data point in the list */</span></div><div class="line">    std::array&lt;int,TOTAL_TRAINING_DATA&gt; train_ids;</div><div class="line">    std::iota(train_ids.begin(),train_ids.end(),0);</div><div class="line"></div><div class="line">    <span class="comment">/* With all this in place we are ready to train the model */</span></div><div class="line">    the_classifier.train( train_ids.cbegin(), train_ids.cend(), training_data_labels.cbegin(), train_feature_lambda, param_lambda, N_DIMS/2 + 1);</div><div class="line"></div><div class="line">    <span class="comment">/* We can now write the model a file for later use */</span></div><div class="line">    the_classifier.writeToFile(FILENAME);</div><div class="line"></div><div class="line">    <span class="comment">/* Generate some unseen test data from the same distributions */</span></div><div class="line">    std::array&lt;std::array&lt;double,N_DIMS&gt;,N_TESTS&gt; test_data_features;</div><div class="line">    std::array&lt;int,N_TESTS&gt; test_data_labels;</div><div class="line">    <span class="keywordflow">for</span>(<span class="keywordtype">unsigned</span> n = 0; n &lt; N_TESTS; ++n)</div><div class="line">    {</div><div class="line">        <span class="comment">/* Choose a random class label */</span></div><div class="line">        <span class="keyword">const</span> <span class="keywordtype">int</span> c = uni_int_dist(rand_engine,std::uniform_int_distribution&lt;int&gt;::param_type{0,N_CLASSES-1});</div><div class="line">        test_data_labels[n] = c;</div><div class="line"></div><div class="line">        <span class="comment">/* Generate some features using this class&#39;s distribution parameters */</span></div><div class="line">        <span class="keywordflow">for</span>(<span class="keywordtype">unsigned</span> d = 0; d &lt; N_DIMS; ++d)</div><div class="line">        {</div><div class="line">            test_data_features[n][d] = norm_dist(rand_engine,std::normal_distribution&lt;double&gt;::param_type{mu[c][d],sigma[c][d]});</div><div class="line">        }</div><div class="line">    }</div><div class="line"></div><div class="line">    <span class="comment">/* We need a way of identifying each of the data points in the test</span></div><div class="line"><span class="comment">    set. This is again done by the index of the data point in the list */</span></div><div class="line">    std::array&lt;int,N_TESTS&gt; test_ids;</div><div class="line">    std::iota(test_ids.begin(),test_ids.end(),0);</div><div class="line"></div><div class="line">    <span class="comment">/* We need a functor to calculate the features for the test set.</span></div><div class="line"><span class="comment">    This is the same as before, but accesses the testing array instead of the</span></div><div class="line"><span class="comment">    training array */</span></div><div class="line">    <span class="keyword">auto</span> test_feature_lambda = [&amp;] (<span class="keyword">auto</span> first_id, <span class="keyword">const</span> <span class="keyword">auto</span> last_id, <span class="keyword">const</span> std::array&lt;int,1&gt;&amp; params, std::vector&lt;float&gt;::iterator out_it)</div><div class="line">    {</div><div class="line">        <span class="keywordflow">while</span>(first_id != last_id)</div><div class="line">        {</div><div class="line">            <span class="keyword">const</span> <span class="keywordtype">int</span> <span class="keywordtype">id</span> = *first_id;</div><div class="line">            <span class="keyword">const</span> <span class="keywordtype">int</span> d = params[0];</div><div class="line">            *out_it++ = test_data_features[id][d];</div><div class="line">            ++first_id;</div><div class="line">        }</div><div class="line">    };</div><div class="line"></div><div class="line">    <span class="comment">/* There are two basic ways to use the forest to analyse test data. The first</span></div><div class="line"><span class="comment">    is to predict the full distribution over the output space given the features.</span></div><div class="line"><span class="comment">    For the classification task, this means finding a discrete distribution over the</span></div><div class="line"><span class="comment">    class labels. First we need to create some discrete distribution objects and</span></div><div class="line"><span class="comment">    initialise them to the right number of classes */</span></div><div class="line">    std::array&lt;canopy::discreteDistribution,N_TESTS&gt; d_dists;</div><div class="line">    <span class="keywordflow">for</span>(<a class="code" href="classcanopy_1_1discrete_distribution.html">canopy::discreteDistribution</a>&amp; dist : d_dists)</div><div class="line">        dist.initialise(N_CLASSES);</div><div class="line"></div><div class="line">    <span class="comment">/* Use the forest model to perform the prediction */</span></div><div class="line">    the_classifier.predictDistGroupwise(test_ids.cbegin(),test_ids.cend(),d_dists.begin(),test_feature_lambda);</div><div class="line"></div><div class="line">    <span class="comment">/* Output the results to console */</span></div><div class="line">    <span class="keywordflow">for</span>(<span class="keywordtype">unsigned</span> n = 0; n &lt; N_TESTS; ++n)</div><div class="line">    {</div><div class="line">        std::cout &lt;&lt; <span class="stringliteral">&quot;True Label &quot;</span> &lt;&lt; test_data_labels[n] &lt;&lt; <span class="stringliteral">&quot;, Predicted Distribution&quot;</span>;</div><div class="line">        <span class="keywordflow">for</span>(<span class="keywordtype">unsigned</span> c = 0; c &lt; N_CLASSES; ++c)</div><div class="line">            std::cout &lt;&lt; <span class="stringliteral">&quot; &quot;</span> &lt;&lt; d_dists[n].pdf(c);</div><div class="line">        std::cout &lt;&lt; std::endl;</div><div class="line">    }</div><div class="line"></div><div class="line">    <span class="comment">/* The other option is to use the forest model to evaluate the probability</span></div><div class="line"><span class="comment">    of a certain value of the label. Suppose we wanted to find the probability</span></div><div class="line"><span class="comment">    under the model that each of the test data belong their ground truth class.</span></div><div class="line"><span class="comment">    This results in floating point value for each test case. */</span></div><div class="line">    std::array&lt;double,N_TESTS&gt; probabilities;</div><div class="line">    the_classifier.probabilityGroupwise(test_ids.cbegin(),test_ids.cend(), test_data_labels.cbegin(), probabilities.begin(), <span class="keyword">false</span>, test_feature_lambda);</div><div class="line"></div><div class="line">    <span class="comment">/* Output the result */</span></div><div class="line">    std::cout &lt;&lt; std::endl &lt;&lt; <span class="stringliteral">&quot;Probabilities:&quot;</span> &lt;&lt; std::endl;</div><div class="line">    <span class="keywordflow">for</span>(<span class="keywordtype">double</span> p : probabilities)</div><div class="line">        std::cout &lt;&lt; p &lt;&lt; std::endl;</div><div class="line">}</div></div><!-- fragment --><p>To build this example in GNU/Linux with GNU make, do the following:</p>
<div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;cd path/to/canopy/example</div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;make</div></div><!-- fragment --><p>To run:</p>
<div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;./canopy_example</div></div><!-- fragment --><p>To clean-up:</p>
<div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;make clean</div></div><!-- fragment --><p>For hints on how to do this in other environments, see <a class="el" href="installation.html#compile">Compiling User Code</a>. </p>
</div></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.11
</small></address>
</body>
</html>
