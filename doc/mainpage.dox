/*!
\mainpage Canopy Tutorial

- \subpage installation "Installation Guide"
- \subpage overview "Overview"
- \subpage existing "Using Existing Models"
- \subpage creating "Creating Your Own Random Forest Model"

*/

/*!
\page installation Installation Guide

\section dependencies Install Dependencies

Firstly you will need to ensure that you have the Canopy's dependencies installed,
this includes:

- \b C++11: A C++ compiler that supports the C++11 standard or later. This includes recent
versions of all the major compilers on all the major platforms. Additionally, you
may find Canopy easier to use if you can use features from the more recent C++14
standard (such as generic lambdas).
- \b OpenMP: A compiler that supports the <A href="http://openmp.org/">OpenMP</A>
standard for multi-threading. Again this includes the major compilers on major
systems. Canopy \b will compile and run in single-threaded mode without this,
but will be much slower.
- \b Boost: The open-source <a href="http://www.boost.org">Boost</a>
<a href="http://www.boost.org/doc/libs/1_62_0/libs/math/doc/html/special.html">
special functions</a> and <a href="http://www.boost.org/doc/libs/1_63_0/libs/iterator/doc/index.html">
iterator</a> libraries. These can be easily installed from package managers
on most GNU/Linux distributions as well as MacPorts or Homebrew on MacOS. Typically
it is easier to install all the Boost libraries at once. E.g. on Ubuntu
\code{bash}
sudo apt-get install libboost-all-dev
\endcode
- \b Eigen: The open-source <a href="http://eigen.tuxfamily.org">Eigen</a>
library for linear algebra. Again this can be easily installed from standard
package managers. E.g. on Ubuntu:
\code{bash}
sudo apt-get install libeigen3-dev
\endcode
If you do not intend to use canopy's \c circularRegressor class, you do not
need to have Eigen installed.

\section get-canopy Get Canopy


Once you have these dependencies installed, you can go ahead and install canopy
by cloning the repository on github. E.g.
\code{bash}
cd /path/where/you/want/canopy
git clone https://github.com/CPBridge/canopy.git
\endcode

And that's it! Since canopy is a header-only library, you don't need to build
anything.

\section compile Compiling User Code

In order to compile your own code using canopy, you need to make sure you are
compiling with c++11 (or later), are using OpenMP, and list canopy's include
directory in the include dependencies. E.g. to compile a programme in \c
 user_code.cpp with the \c g++ compiler:
\code{bash}
g++ -std=c++11 -fopenmp -I /path/to/canopy/include user_code.cpp
\endcode
*/

/*!
\page overview Overview

\section forests Random Forests

In order to describe how the canopy library works, we will first need an abstract
description of the random forests algorithm. I recommend the following as an
excellent thorough introduction:

- A. Criminisi, J. Shotton and E. Konukoglu. <a href="https://www.microsoft.com/en-us/research/publication/decision-forests-for-classification-regression-density-estimation-manifold-learning-and-semi-supervised-learning/">Decision Forests for Classification,
Regression, Density Estimation, Manifold Learning and Semi-Supervised Learning</a>.
Technical Report. Microsoft Research.

A random forest is a collection of binary decision trees that is used to predict
some quantity based on some <b>input features</b>. We will refer to the quantity
being predicted as a <b>label</b>. The nature of the label will vary with the
task being performed. For example, in a classification task, the label wil be a
discrete (integer) variable representing the class. For a regression problem, the
label will be a continuous quantity (represented by a floating point number), etc.
The features can be similarly general, they could be a set of pre-recorded
measurements, or the result of some function applied to an image, video, or signal.

A data point is passed into the <b>root node</b> of each tree in the forest, and
is passed down the tree until it reaches a <b>leaf node</b>. Each node looks at
the values of some of the features in order to decide whether to pass the data
point down to its left or right <b>child node</b>. This repeats until the leaf
node, which has no children, is reached. The leaf node contains some distribution
over the value of the label given the input, we will call such a distribution a
<b>node distribution</b>.

There are two tasks that the forest models created using canopy can perform at this
point:

- Combine the <b>node distributions</b> reached in each of the trees to give a
new distribution over the value of the label. We will call this distribution an
<b>output distribution</b>. In general its form may be the same as the <b>node
distribution</b> or it may be different.
We will call this the <b>distribution prediction</b> task.
- Evaluate the probability of a certain value of the label variable. This is done
by using the <b>node distributions</b> reached in each tree to evaluate the
probability of that label, and then averaging this result over all the trees.
We will call this the <b>probability evaluation</b> task.

In order to train each tree in the forests, a randomly selected set of features
are tested to see which can split the labels in the training set to give the
<b>pureest</b> child nodes. The concrete definition of <b>purity</b> depends on
the type of the labels and the particular problem at hand.

\section randomForestBase The randomForestBase Class

The canopy library uses a single base class called \c randomForestBase to provide
the general framework for training and testing random forests as described in
the previous section.

The \c randomForestBase class looks something like this:

\code{.cpp}
template <class TDerived, class TLabel, class TNodeDist, class TOutputDist, unsigned TNumParams>
class randomForestBase
{
	//...
};
\endcode

There are several template parameters that allow this classes derived from this base class to implement a wide range of different behaviours:

- <b>TDerived</b> This is  the type of the derived class, needed in order to use the CRTP form of static polymorphism. 
- <b>TLabel</b> The type of the <b>label</b> (the variable to be predicted). This can in principle be an arbitrary type.
- <b>TNodeDist</b> This is the type of the <b>node distribution</b>, which must have a specific form dictated by the choice of <b>TLabel</b>.
- <b>TOutputDist</b> This is the type of the <b>output distribution</b>, which must have a specific form dictated by the choice of <b>TNodeDist</b>.
- <b>TNumParams</b> This is the number of parameters of the feature callback function.

Specific random forest models may be derived from this base class using CRTP with a specific set 
of these template parameters (although TNumParams is typically left unspecified). 

Canopy preovides two pre-defined random forest models:

- <b>classifier</b> A random forest classifier to predict a discrete label (TLabel = int) 
with a discreteDistribution object serving as the node and output distribution type
(TNodeDist = discreteDistribution and TOutputDist = discreteDistribution).
- <b>circularRegressor</b>  A random forest model for predicting a circular-valued
(angular) output variable, represented using a floating point number (TLabel = float)
and using a vonMisesDistribution object as the node and output distribution type.

\section functors Defining Feature Functors

Canopy handles features using functor objects in order to give maximum flexibility
for the feature calculation process to execute arbitrary code and access arbitrary data.

There are two different forms of feature functor that are accepted by canopy under
different circumstances. The difference between these two forms is how they 
handle multiple test data.

In both cases, the purpose of the functor object is to take some test points 
identified by some ID type, apply some function to them specified by some 
number of integer-valued parameters, and return a single floating-point
value for each input ID as the result.

The ID can be any arbitrary data type used to identify a test data point.
The ID variables are not used or moved around by Canopy methods, so
it really doesn't matter what this is as long as your own functors can make
sense of it. In the simplest case (as in the provided example), the ID is simply 
an integer representing the index of the test sample in some list. However, it
could be more complicated than this, for example it could be a 2D vector 
representing an image location in an image, or a 3D vector representing
a 3D location within a volume image.

The function that the functor applies to each test data point can be parameterised
by an arbitrary number of integer-valued parameters, which are passed to the 
functor by const reference in a std::array. The number of parameters is controlled 
by the TNumParams template parameter of the base class. Again it is up to you to 
define the meaning to these parameters, the Canopy code just passes them around
without using them.

The first of the two forms calculates the features for test data in a one-by-one fashion.
We will refer to this form as the <b>Single Feature Functor</b> form.
This is the simplest option and involves the smallest overhead.

The form of this type is:

\code{.cpp}
\endcode

The second form allows for calculating the features for multiple test points
together. We will refer to this as the <b><Groupwise Feature Functor</b> form. 
This can be advantageous when such a process can be made quicker 
than evaluating the test data individually. For example, if the test data are different
image locations in the same image, then it may be faster the perform some process
(e.g. an FFT) on the entire image than work on each point individually.

In both cases, the functor must provide the paranthesis operator, operator(), 
of the form above. There are two ways to do this. The first is to define a custom 
class or struct with and define the operator(). This allows you to encapsulate 
the feature extraction process along with all its data in a single object.

The second option is to use a C++11 lambda function. The advantage of this
approach is that it can easily access data defined elsewhere in your programme
via lambda captures. However, with C++11, you will need to manually work out
all the types of the the operator() parameters, which ends up being quite complicated
due to the iterator trickery buried within Canopy. Luckily however , the C++14 standard
fixes this problem by allowing you to define generic lambdas using the auto keyword 
in place of the types. 

\section traning Training A Forest Model

In order to train a model you must create a <b>Groupwise Feature Functor</b> object 
and pass it to the randomForestBase's train() method. 

Additionally, you must define a second functor object that generates valid combination of 
parameters for your split functors on demand. Which combinations are valid depends entirely
on the feature calculation process that you are using.

The <b>Parameter Generator Functor</b> must take the following form:  

*/

/*!
\page existing "Using Existing Models"
*/

/*!
\page creating "Creating Your Own Random Forest Model"
*/